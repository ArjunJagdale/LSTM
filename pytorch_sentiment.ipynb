{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHUyEof0j4XH"
      },
      "source": [
        "### Twitter US Airline Sentiment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import re\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "cm66t2TeQr-P"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading IMDB dataset...\")\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTxksl4HQsQK",
        "outputId": "31ee7ef9-6fbc-47d0-961e-06f7ccd72a7b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading IMDB dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "dataset = load_dataset(\"imdb\")\n",
        "train_data = dataset[\"train\"]\n",
        "test_data = dataset[\"test\"]\n",
        "\n",
        "print(f\"Train size: {len(train_data)}\")\n",
        "print(f\"Test size: {len(test_data)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdvzPtOwQtvW",
        "outputId": "2992edc9-b3c6-4af3-ccd1-af51f72394a6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 25000\n",
            "Test size: 25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"Clean and preprocess text\"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'<br />', ' ', text)  # Remove HTML breaks\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Keep only letters and spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "4iobguPhQwtU"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "    \"\"\"Simple tokenization\"\"\"\n",
        "    return clean_text(text).split()\n",
        "\n",
        "print(\"\\nBuilding vocabulary...\")\n",
        "# Build vocabulary from training data only\n",
        "all_tokens = []\n",
        "for text in tqdm(train_data['text'][:5000], desc=\"Processing texts\"):  # Use subset for faster vocab building\n",
        "    all_tokens.extend(tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nX4LmRl2Qyy7",
        "outputId": "a0139a5d-439c-4058-f738-cb65990f4764"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Building vocabulary...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing texts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:00<00:00, 8238.02it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_counts = Counter(all_tokens)\n",
        "# Keep tokens that appear at least 5 times\n",
        "vocab_tokens = [word for word, count in token_counts.most_common() if count >= 5]\n",
        "vocab_tokens = vocab_tokens[:10000]  # Keep top 10k words"
      ],
      "metadata": {
        "id": "9jP9LoYgQ060"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create vocabulary\n",
        "vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "vocab.update({word: idx + 2 for idx, word in enumerate(vocab_tokens)})\n",
        "\n",
        "print(f\"Vocabulary size: {len(vocab)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dh_9j7sQ6ec",
        "outputId": "a4f63a28-99a0-46f6-a880-1f3bdb40ec03"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 10002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, texts, labels, vocab, max_len=256):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.vocab = vocab\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Tokenize and encode\n",
        "        tokens = tokenize(text)[:self.max_len]\n",
        "        encoded = [self.vocab.get(token, self.vocab[\"<UNK>\"]) for token in tokens]\n",
        "\n",
        "        # Pad or truncate\n",
        "        if len(encoded) < self.max_len:\n",
        "            encoded = encoded + [self.vocab[\"<PAD>\"]] * (self.max_len - len(encoded))\n",
        "        else:\n",
        "            encoded = encoded[:self.max_len]\n",
        "\n",
        "        return torch.tensor(encoded, dtype=torch.long), torch.tensor(label, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "WkkP7imVQ74X"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets\n",
        "print(\"\\nCreating datasets...\")\n",
        "train_dataset = IMDBDataset(train_data['text'], train_data['label'], vocab)\n",
        "test_dataset = IMDBDataset(test_data['text'], test_data['label'], vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eKRKrR6Q-JH",
        "outputId": "0410bc00-7d01-4c7a-9a67-093957f7cb1d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Creating datasets...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataloaders\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
      ],
      "metadata": {
        "id": "d0cSeikhRAFo"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Test batches: {len(test_loader)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpiO2q5mRCNW",
        "outputId": "a1b523b8-1809-4fad-bd6c-6e143a048741"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train batches: 391\n",
            "Test batches: 391\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, num_layers=2, dropout=0.5):\n",
        "        super(SentimentLSTM, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers,\n",
        "                           batch_first=True, dropout=dropout if num_layers > 1 else 0,\n",
        "                           bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, 1)  # *2 for bidirectional\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, seq_len)\n",
        "        embedded = self.embedding(x)  # (batch_size, seq_len, embedding_dim)\n",
        "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
        "\n",
        "        # Concatenate last hidden states from both directions\n",
        "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
        "        hidden = self.dropout(hidden)\n",
        "\n",
        "        output = self.fc(hidden)\n",
        "        return output.squeeze()\n"
      ],
      "metadata": {
        "id": "FpUHdrhvREl-"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"\\nUsing device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U33GH1MURG-V",
        "outputId": "51696483-0c01-41f8-8348-f6f23950c8c2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentimentLSTM(len(vocab)).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "print(\"\\nModel architecture:\")\n",
        "print(model)\n",
        "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfeN1jm8RIrF",
        "outputId": "7d4d1b4a-e339-4dac-805c-efec0be575e2"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model architecture:\n",
            "SentimentLSTM(\n",
            "  (embedding): Embedding(10002, 128, padding_idx=0)\n",
            "  (lstm): LSTM(128, 256, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
            "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n",
            "\n",
            "Total parameters: 3,648,257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for texts, labels in tqdm(dataloader, desc=\"Training\"):\n",
        "        texts, labels = texts.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(texts)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        predictions = (torch.sigmoid(outputs) > 0.5).float()\n",
        "        correct += (predictions == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    return total_loss / len(dataloader), correct / total"
      ],
      "metadata": {
        "id": "Ruw9DM1nRLEB"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for texts, labels in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            texts, labels = texts.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(texts)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            predictions = (torch.sigmoid(outputs) > 0.5).float()\n",
        "            correct += (predictions == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    return total_loss / len(dataloader), correct / total"
      ],
      "metadata": {
        "id": "RRuV9reBRN1C"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING START\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "num_epochs = 5\n",
        "best_test_acc = 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W76TnoRmRPxi",
        "outputId": "0f16c0f1-f052-4f35-9d0b-9412b95b9423"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "TRAINING START\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
        "    print(f\"Test Loss:  {test_loss:.4f} | Test Acc:  {test_acc*100:.2f}%\")\n",
        "\n",
        "    if test_acc > best_test_acc:\n",
        "        best_test_acc = test_acc\n",
        "        torch.save(model.state_dict(), 'best_sentiment_model.pth')\n",
        "        print(f\"âœ“ New best model saved! (Acc: {test_acc*100:.2f}%)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"TRAINING COMPLETE - Best Test Accuracy: {best_test_acc*100:.2f}%\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sC02c5UrRRuy",
        "outputId": "e21bc891-ed06-4308-f321-f4631fdb11ca"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/5\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:50<00:00,  7.74it/s]\n",
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:27<00:00, 14.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.6629 | Train Acc: 59.47%\n",
            "Test Loss:  0.6494 | Test Acc:  66.78%\n",
            "âœ“ New best model saved! (Acc: 66.78%)\n",
            "\n",
            "Epoch 2/5\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:50<00:00,  7.72it/s]\n",
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:26<00:00, 14.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5205 | Train Acc: 75.30%\n",
            "Test Loss:  0.4637 | Test Acc:  78.54%\n",
            "âœ“ New best model saved! (Acc: 78.54%)\n",
            "\n",
            "Epoch 3/5\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:50<00:00,  7.69it/s]\n",
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:27<00:00, 14.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4118 | Train Acc: 82.79%\n",
            "Test Loss:  0.4489 | Test Acc:  81.70%\n",
            "âœ“ New best model saved! (Acc: 81.70%)\n",
            "\n",
            "Epoch 4/5\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:51<00:00,  7.63it/s]\n",
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:27<00:00, 14.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.3463 | Train Acc: 86.10%\n",
            "Test Loss:  0.3914 | Test Acc:  83.52%\n",
            "âœ“ New best model saved! (Acc: 83.52%)\n",
            "\n",
            "Epoch 5/5\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:51<00:00,  7.63it/s]\n",
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:27<00:00, 14.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2845 | Train Acc: 88.84%\n",
            "Test Loss:  0.3822 | Test Acc:  85.19%\n",
            "âœ“ New best model saved! (Acc: 85.19%)\n",
            "\n",
            "============================================================\n",
            "TRAINING COMPLETE - Best Test Accuracy: 85.19%\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(text, model, vocab, device, max_len=256):\n",
        "    \"\"\"Predict sentiment for a given text\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Preprocess\n",
        "    tokens = tokenize(text)[:max_len]\n",
        "    encoded = [vocab.get(token, vocab[\"<UNK>\"]) for token in tokens]\n",
        "\n",
        "    # Pad\n",
        "    if len(encoded) < max_len:\n",
        "        encoded = encoded + [vocab[\"<PAD>\"]] * (max_len - len(encoded))\n",
        "\n",
        "    # Convert to tensor\n",
        "    text_tensor = torch.tensor(encoded, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    # Predict\n",
        "    with torch.no_grad():\n",
        "        output = model(text_tensor)\n",
        "        probability = torch.sigmoid(output).item()\n",
        "\n",
        "    sentiment = \"Positive\" if probability > 0.5 else \"Negative\"\n",
        "    confidence = probability if probability > 0.5 else 1 - probability\n",
        "\n",
        "    return sentiment, confidence, probability"
      ],
      "metadata": {
        "id": "ohKJ1xJ1RT8C"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print detailed parameter breakdown\n",
        "print(\"\\nDetailed Model Parameters:\")\n",
        "print(\"=\"*60)\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"{name:30s} | Shape: {str(list(param.shape)):25s} | Params: {param.numel():,}\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AiWUC2nWx3N",
        "outputId": "dabc4cfb-2592-48a7-aea8-fba1960b3db2"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Detailed Model Parameters:\n",
            "============================================================\n",
            "embedding.weight               | Shape: [10002, 128]              | Params: 1,280,256\n",
            "lstm.weight_ih_l0              | Shape: [1024, 128]               | Params: 131,072\n",
            "lstm.weight_hh_l0              | Shape: [1024, 256]               | Params: 262,144\n",
            "lstm.bias_ih_l0                | Shape: [1024]                    | Params: 1,024\n",
            "lstm.bias_hh_l0                | Shape: [1024]                    | Params: 1,024\n",
            "lstm.weight_ih_l0_reverse      | Shape: [1024, 128]               | Params: 131,072\n",
            "lstm.weight_hh_l0_reverse      | Shape: [1024, 256]               | Params: 262,144\n",
            "lstm.bias_ih_l0_reverse        | Shape: [1024]                    | Params: 1,024\n",
            "lstm.bias_hh_l0_reverse        | Shape: [1024]                    | Params: 1,024\n",
            "lstm.weight_ih_l1              | Shape: [1024, 512]               | Params: 524,288\n",
            "lstm.weight_hh_l1              | Shape: [1024, 256]               | Params: 262,144\n",
            "lstm.bias_ih_l1                | Shape: [1024]                    | Params: 1,024\n",
            "lstm.bias_hh_l1                | Shape: [1024]                    | Params: 1,024\n",
            "lstm.weight_ih_l1_reverse      | Shape: [1024, 512]               | Params: 524,288\n",
            "lstm.weight_hh_l1_reverse      | Shape: [1024, 256]               | Params: 262,144\n",
            "lstm.bias_ih_l1_reverse        | Shape: [1024]                    | Params: 1,024\n",
            "lstm.bias_hh_l1_reverse        | Shape: [1024]                    | Params: 1,024\n",
            "fc.weight                      | Shape: [1, 512]                  | Params: 512\n",
            "fc.bias                        | Shape: [1]                       | Params: 1\n",
            "============================================================\n",
            "Total parameters: 3,648,257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Final epoch training metrics\n",
        "print(f\"\\nFinal Training Metrics:\")\n",
        "print(f\"Train Loss: {train_loss:.4f}\")\n",
        "print(f\"Train Accuracy: {train_acc*100:.2f}%\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
        "\n",
        "# 2. Training time (add at start and end of training)\n",
        "import time\n",
        "start_time = time.time()\n",
        "# ... your training loop ...\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "print(f\"\\nTotal training time: {training_time/60:.2f} minutes\")\n",
        "\n",
        "# 3. Test a few sample predictions\n",
        "samples = [\n",
        "    \"This movie was absolutely fantastic!\",\n",
        "    \"Terrible waste of time and money.\",\n",
        "    \"It was okay, nothing special.\"\n",
        "]\n",
        "print(\"\\nSample Predictions:\")\n",
        "for text in samples:\n",
        "    sentiment, confidence, prob = predict_sentiment(text, model, vocab, device)\n",
        "    print(f\"Text: {text[:50]}\")\n",
        "    print(f\"  â†’ {sentiment} ({confidence*100:.1f}%)\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fh_rhuetYMFC",
        "outputId": "977a5395-1c36-4868-a87d-0cdcc4160fc0"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Training Metrics:\n",
            "Train Loss: 0.2845\n",
            "Train Accuracy: 88.84%\n",
            "Test Loss: 0.3822\n",
            "Test Accuracy: 85.19%\n",
            "\n",
            "Total training time: 0.00 minutes\n",
            "\n",
            "Sample Predictions:\n",
            "Text: This movie was absolutely fantastic!\n",
            "  â†’ Positive (84.5%)\n",
            "\n",
            "Text: Terrible waste of time and money.\n",
            "  â†’ Negative (95.4%)\n",
            "\n",
            "Text: It was okay, nothing special.\n",
            "  â†’ Negative (81.4%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nModel Architecture:\")\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEmy-0k6YkYO",
        "outputId": "4ef26f1c-1e41-4d97-cbce-4b184df9930e"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Architecture:\n",
            "SentimentLSTM(\n",
            "  (embedding): Embedding(10002, 128, padding_idx=0)\n",
            "  (lstm): LSTM(128, 256, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
            "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"SAMPLE PREDICTIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "test_reviews = [\n",
        "    \"This movie was absolutely amazing! Best film I've seen all year.\",\n",
        "    \"Terrible waste of time. The plot made no sense and acting was horrible.\",\n",
        "    \"It was okay, nothing special but not terrible either.\",\n",
        "    \"A masterpiece of cinema. Beautiful cinematography and compelling story.\",\n",
        "    \"I fell asleep halfway through. Boring and predictable.\",\n",
        "    \"The tables were too short. The food was best!\"\n",
        "]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfGFRe6SRZ5M",
        "outputId": "3ff2f9b0-d3d9-4fdf-9f3a-ff67c4db72ba"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SAMPLE PREDICTIONS\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for review in test_reviews:\n",
        "    sentiment, confidence, prob = predict_sentiment(review, model, vocab, device)\n",
        "    print(f\"\\nReview: {review[:60]}...\")\n",
        "    print(f\"Sentiment: {sentiment} (Confidence: {confidence*100:.1f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WYRNlMo_Rbf8",
        "outputId": "e95541b1-e137-4a5b-d0eb-fd6d6921b3c8"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Review: This movie was absolutely amazing! Best film I've seen all y...\n",
            "Sentiment: Positive (Confidence: 98.0%)\n",
            "\n",
            "Review: Terrible waste of time. The plot made no sense and acting wa...\n",
            "Sentiment: Negative (Confidence: 98.4%)\n",
            "\n",
            "Review: It was okay, nothing special but not terrible either....\n",
            "Sentiment: Negative (Confidence: 93.9%)\n",
            "\n",
            "Review: A masterpiece of cinema. Beautiful cinematography and compel...\n",
            "Sentiment: Positive (Confidence: 95.0%)\n",
            "\n",
            "Review: I fell asleep halfway through. Boring and predictable....\n",
            "Sentiment: Negative (Confidence: 98.2%)\n",
            "\n",
            "Review: The tables were too short. The food was best!...\n",
            "Sentiment: Positive (Confidence: 65.5%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    \"\"\"Analyze sentiment and return probabilities\"\"\"\n",
        "    if not text.strip():\n",
        "        return {\"Negative\": 0.5, \"Positive\": 0.5}\n",
        "\n",
        "    sentiment, confidence, prob = predict_sentiment(text, model, vocab, device)\n",
        "\n",
        "    # Return probabilities for both classes\n",
        "    return {\n",
        "        \"Negative\": 1 - prob,\n",
        "        \"Positive\": prob\n",
        "    }\n",
        "\n",
        "# Example reviews\n",
        "examples = [\n",
        "    [\"This movie was absolutely fantastic! The acting was superb and I loved every minute of it.\"],\n",
        "    [\"Worst movie ever. Complete waste of time and money. Terrible acting and boring plot.\"],\n",
        "    [\"It was okay. Some good parts, some bad parts. Nothing particularly memorable.\"],\n",
        "    [\"A masterpiece! Beautiful cinematography, amazing soundtrack, and powerful performances.\"],\n",
        "    [\"I fell asleep halfway through. Slow, boring, and predictable storyline.\"]\n",
        "]\n"
      ],
      "metadata": {
        "id": "oPdFCZo4Ua1P"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Gradio interface\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"LAUNCHING GRADIO INTERFACE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# ðŸŽ¬ IMDB Movie Review Sentiment Analyzer\")\n",
        "    gr.Markdown(\"Enter a movie review to analyze its sentiment using our LSTM model trained on 25,000 IMDB reviews.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            text_input = gr.Textbox(\n",
        "                label=\"Movie Review\",\n",
        "                placeholder=\"Enter your movie review here...\",\n",
        "                lines=5\n",
        "            )\n",
        "            submit_btn = gr.Button(\"Analyze Sentiment\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "        with gr.Column():\n",
        "            output = gr.Label(\n",
        "                label=\"Sentiment Analysis\",\n",
        "                num_top_classes=2\n",
        "            )\n",
        "\n",
        "    gr.Examples(\n",
        "        examples=examples,\n",
        "        inputs=text_input,\n",
        "        outputs=output,\n",
        "        fn=analyze_sentiment,\n",
        "        cache_examples=False\n",
        "    )\n",
        "\n",
        "    submit_btn.click(\n",
        "        fn=analyze_sentiment,\n",
        "        inputs=text_input,\n",
        "        outputs=output\n",
        "    )\n",
        "\n",
        "    text_input.submit(\n",
        "        fn=analyze_sentiment,\n",
        "        inputs=text_input,\n",
        "        outputs=output\n",
        "    )\n",
        "\n",
        "demo.launch(share=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "jHNfoz4rUbKz",
        "outputId": "10f68ec8-68cf-4f88-d8a2-63035060164c"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "LAUNCHING GRADIO INTERFACE\n",
            "============================================================\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7862, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "wwBylUZlOW3X",
        "ugA-KPFJOdxk",
        "gHUyEof0j4XH"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}